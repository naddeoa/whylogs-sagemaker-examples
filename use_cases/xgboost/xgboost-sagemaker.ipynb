{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://whylabs-public.s3.us-west-2.amazonaws.com/assets/whylabs-logo-night-blue.svg)\n",
    "\n",
    "*Run AI with Certainty*\n",
    "\n",
    "# **Using WhyLabs with Sagemaker** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch installed elsewhere\n",
    "%pip install sagemaker xgboost python-dotenv ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just storing sensitive stuff in a .env file.\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Create a sagemaker.env file with these vars\n",
    "# SAGEMAKER_ROLE=\n",
    "# WHYLABS_API_KEY=\n",
    "# WHYLABS_DEFAULT_DATASET_ID=\n",
    "# BUCKET_ENV=\n",
    "\n",
    "load_dotenv(dotenv_path='sagemaker.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Authentication\n",
    "\n",
    "Set up the AWS authentication by preparing an execution role for Sagemaker and ensuring you can use the aws cli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "# A sagemaker execution role that you previously created\n",
    "aws_role = os.getenv(\"SAGEMAKER_ROLE\")\n",
    "aws_region = \"us-west-2\"\n",
    "session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n",
      "Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Convert the dataset into the DMatrix format used by XGBoost\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Set up the parameters for XGBoost\n",
    "# Objective is multi:softmax for multi-class classification problems\n",
    "# num_class should be set to the number of classes to predict\n",
    "# There are many other parameters you can set to customize the boosting process\n",
    "param = {\n",
    "    'max_depth': 3,  # Maximum depth of a tree\n",
    "    'eta': 0.3,      # Learning rate\n",
    "    'objective': 'multi:softmax',  # Multiclass classification problem\n",
    "    'num_class': 3}  # Number of classes in objective\n",
    "num_round = 20  # Number of boosting rounds\n",
    "\n",
    "# df = pd.DataFrame(X_train)\n",
    "\n",
    "# Train the model\n",
    "bst = xgb.train(param, dtrain, num_round)\n",
    "\n",
    "\n",
    "print(y_test)\n",
    "# Make predictions\n",
    "preds = bst.predict(dtest)\n",
    "\n",
    "# Evaluate the predictions\n",
    "accuracy = accuracy_score(y_test, preds)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare model\n",
    "For this example we'll package up an existing model, one of the resnet variants available in pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anthony/workspace/sagemaker-example/.venv/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [13:27:18] WARNING: /workspace/src/c_api/c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model artifact uploaded to: s3://sagemaker-us-west-2-207285235248/sagemaker_models/xgboost-iris/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "\n",
    "model_file_name = \"xgboost-model\"\n",
    "bst.save_model(model_file_name)\n",
    "\n",
    "# Set up the path in the bucket\n",
    "bucket = os.getenv(\"BUCKET_NAME\")\n",
    "key_prefix = 'sagemaker_models/xgboost-iris'\n",
    "\n",
    "\n",
    "# Then, compress it into a tar.gz file\n",
    "model_archive_name = \"model.tar.gz\"\n",
    "with tarfile.open(model_archive_name, \"w:gz\") as tar:\n",
    "    tar.add(model_file_name)\n",
    "\n",
    "upload_path = session.upload_data(path=model_archive_name, bucket=bucket, key_prefix=key_prefix)\n",
    "print(f\"Model artifact uploaded to: {upload_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the requirements file\n",
    "You'll need to install whylogs on the Sagemaker host. You do this by passing a requirements file with everything that you need. We'll create a dummy virtual env here just to export a requirements file for Sagemaker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just bundled with the requirements.txt file instead of creating dynamically\n",
    "\n",
    "# ! mkdir -p code \n",
    "# ! bash -c \"virtualenv ./code/.venv && source ./code/.venv/bin/activate && pip install xgboost==1.7.6 whylogs[proc]==1.3.8 && pip freeze > code/requirements.txt\"\n",
    "# ! rm -rf ./code/.venv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an inference.py file\n",
    "The integration happens in the custom inference logic for the Sagemaker container. The important parts are captured below.  This cell will be written to a file and deployed along with the model further down. This happens to be logging image data but it works with other kinds of data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/inference.py\n",
    "import multiprocessing\n",
    "import os\n",
    "import traceback\n",
    "import json\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "\n",
    "import whylogs as why\n",
    "from whylogs.api.writer import Writer, Writers\n",
    "from whylogs.api.logger.experimental.logger.actor.thread_rolling_logger import ThreadRollingLogger\n",
    "from whylogs.api.logger.experimental.logger.actor.time_util import Schedule, TimeGranularity\n",
    "\n",
    "# Initialize whylogs with your WhyLabs API key and target dataset ID. You can get an api key from the\n",
    "# settings menu of you WhyLabs account.\n",
    "why.init() # This loads credentials from the env directly\n",
    "\n",
    "def create_logger():\n",
    "    logger = ThreadRollingLogger(\n",
    "        # This should match the model type in WhyLabs. We're using a daily model here.\n",
    "        aggregate_by=TimeGranularity.Day,\n",
    "        # The profiles will be uploaded from the rolling logger to WhyLabs every 5 minutes. Data\n",
    "        # will accumulates during that time.\n",
    "        write_schedule=Schedule(cadence=TimeGranularity.Minute, interval=5),\n",
    "        writers=[Writers.get('whylabs')]\n",
    "    )\n",
    "\n",
    "    return logger\n",
    "\n",
    "logger = create_logger()\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    model_file = \"xgboost-model\"\n",
    "    booster = xgb.Booster()\n",
    "    booster.load_model(os.path.join(model_dir, model_file))\n",
    "    return booster\n",
    "\n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "    print(f'Logger closed: {logger.is_closed()}, alive : {logger.is_alive()}')\n",
    "\n",
    "    assert request_content_type == 'application/json'\n",
    "    # Body should be a list of lists of length 4\n",
    "    body = json.loads(request_body)\n",
    "\n",
    "    if 'flush' in body and body['flush']:\n",
    "        # Utility for flushing the logger, which forces it to upload any pending profiles synchronously.\n",
    "        print(\"Flushing logger...\")\n",
    "        logger.flush()\n",
    "        print(\"Done flushing logger\")\n",
    "        return None\n",
    "\n",
    "    if 'close' in body and body['close']:\n",
    "        logger.close()\n",
    "        return None\n",
    "\n",
    "    if type(body) is not list:\n",
    "        raise ValueError(f\"Expected a list of lists, got {type(body)}\")\n",
    "\n",
    "    if len(body) == 0:\n",
    "        raise ValueError(\"Expected a list of lists, got an empty list\")\n",
    "\n",
    "    if len(body[0]) != 4:\n",
    "        raise ValueError(f\"Expected a list of lists of length 4, got a list of lists of length {len(body[0])}\")\n",
    "\n",
    "    return body\n",
    "\n",
    "\n",
    "column_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "\n",
    "def predict_fn(input_data: List[List[float]], model):\n",
    "    if input_data is None:\n",
    "        return \"\"\n",
    "\n",
    "    test = xgb.DMatrix(input_data)\n",
    "    predictions = model.predict(test)\n",
    "\n",
    "    df = pd.DataFrame(input_data, columns=column_names)\n",
    "    df['prediction'] = predictions.tolist()\n",
    "\n",
    "    try:\n",
    "        print(\"Logging prediction...\")\n",
    "        logger.log(df)\n",
    "        print(\"Done logging prediction\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to log prediction: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "    return predictions.tolist()\n",
    "\n",
    "\n",
    "def output_fn(prediction, content_type):\n",
    "    return json.dumps(prediction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a XGBoost deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.xgboost import XGBoostModel\n",
    "\n",
    "sagemaker_model = XGBoostModel(\n",
    "    source_dir='code',\n",
    "    entry_point='inference.py',\n",
    "    model_data=upload_path,\n",
    "    framework_version='1.7-1',\n",
    "    role=aws_role,\n",
    "    env={\n",
    "        'WHYLABS_API_KEY': os.environ['WHYLABS_API_KEY'],\n",
    "        'WHYLABS_DEFAULT_DATASET_ID': os.environ['WHYLABS_DEFAULT_DATASET_ID']\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----!"
     ]
    }
   ],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import StringDeserializer\n",
    "\n",
    "predictor = sagemaker_model.deploy(initial_instance_count=1, instance_type='ml.m5.large')\n",
    "predictor.serializer = JSONSerializer()\n",
    "predictor.deserializer = StringDeserializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data):\n",
    "    return predictor.predict(data, initial_args={'ContentType': 'application/json'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[1.0, 0.0, 2.0, 1.0, 1.0, 0.0, 1.0, 2.0, 1.0, 1.0, 2.0, 0.0, 0.0, 0.0, 0.0, 1.0, 2.0, 1.0, 1.0, 2.0, 0.0, 2.0, 0.0, 2.0, 2.0, 2.0, 2.0, 2.0, 0.0, 0.0]'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict with our test data\n",
    "\n",
    "predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Force the logger to upload\n",
    "\n",
    "> ⚠️ These things really only reliably work if you have a single instance behind your prediction endpoint. Otherwise you'll have to make sure these requests get to each endpoint individually.\n",
    "\n",
    "This forces the logger to upload (see the inference.py code) which uploads any remaining data in the logger before we close down the Sagemaker endpoint. The rolling logger typically uploads data on a predefined interval so you can do something like this to make sure you don't clip your profile uploads before shutting things down. Sagemaker doesn't provide any \"on close\" hooks to make this transparent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\"'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict({'flush':True}, initial_args={'ContentType': 'application/json'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, you can close the logger. This also forces an upload but results in the logger no longer being active, so you should only do this before you're about to tear down the endpoint since it let's you synchronously wait for any pending uploads to finish.\n",
    "\n",
    "Remember, these don't automatically work. They only work because we set up the `inference.py`` file to check for these payloads and call the right methods on the logger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor.predict({'close':True}, initial_args={'ContentType': 'application/json'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
