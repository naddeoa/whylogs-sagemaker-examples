{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://whylabs-public.s3.us-west-2.amazonaws.com/assets/whylabs-logo-night-blue.svg)\n",
    "\n",
    "*Run AI with Certainty*\n",
    "\n",
    "# **Using WhyLabs with Sagemaker** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sagemaker torch torchvision transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS Authentication\n",
    "\n",
    "Set up the AWS authentication by preparing an execution role for Sagemaker and ensuring you can use the aws cli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just storing sensitive stuff in a .env file.\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Create a sagemaker.env file with these vars\n",
    "# SAGEMAKER_ROLE=\n",
    "# WHYLABS_API_KEY=\n",
    "# WHYLABS_DEFAULT_DATASET_ID=\n",
    "# BUCKET_ENV=\n",
    "\n",
    "load_dotenv(dotenv_path='sagemaker.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "# A sagemaker execution role that you previously created\n",
    "aws_role = os.getenv(\"SAGEMAKER_ROLE\")\n",
    "aws_region = \"us-west-2\"\n",
    "session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare model\n",
    "For this example we'll package up an existing model, one of the resnet variants available in pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import tarfile\n",
    "import os\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "model_path = 'model/'\n",
    "code_path = 'code/'\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.mkdir(model_path)\n",
    "    \n",
    "model.save_pretrained(save_directory=model_path)\n",
    "tokenizer.save_vocabulary(save_directory=model_path)\n",
    "\n",
    "bucket = os.getenv(\"BUCKET_NAME\")\n",
    "key_prefix = 'sagemaker_models/gpt2'\n",
    "\n",
    "with tarfile.open(os.path.join(model_path, \"model.tar.gz\"), 'w:gz') as tar:\n",
    "    tar.add(model_path)\n",
    "    # tar.add(code_path)\n",
    "\n",
    "upload_path = session.upload_data(path='model/model.tar.gz', bucket=bucket, key_prefix=key_prefix)\n",
    "# int(f\"Model artifact uploaded to: {upload_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the requirements file\n",
    "You'll need to install whylogs on the Sagemaker host. You do this by passing a requirements file with everything that you need. We'll create a dummy virtual env here just to export a requirements file for Sagemaker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just bundled with the requirements.txt file instead of creating dynamically\n",
    "\n",
    "# ! mkdir -p code \n",
    "# ! bash -c \"virtualenv ./code/.venv && source ./code/.venv/bin/activate && pip install whylogs[image,proc]==1.3.8 langkit[all] && pip freeze > code/requirements.txt\"\n",
    "# ! rm -rf ./code/.venv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an inference.py file\n",
    "The integration happens in the custom inference logic for the Sagemaker container. The important parts are captured below.  This cell will be written to a file and deployed along with the model further down. This happens to be logging image data but it works with other kinds of data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/inference.py\n",
    "import traceback\n",
    "import torch\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "import whylogs as why\n",
    "from whylogs.api.logger.experimental.logger.actor.process_rolling_logger import ProcessRollingLogger\n",
    "from langkit import llm_metrics # alternatively use 'light_metrics'\n",
    "from whylogs.api.logger.experimental.logger.actor.time_util import Schedule, TimeGranularity\n",
    "from transformers import GPT2Tokenizer, TextGenerationPipeline, GPT2LMHeadModel\n",
    "\n",
    "\n",
    "# Initialize whylogs with your WhyLabs API key and target dataset ID. You can get an api key from the\n",
    "# settings menu of you WhyLabs account. \n",
    "why.init() # This loads credentials from the env directly\n",
    "row_name = \"image\"\n",
    "\n",
    "def create_logger():\n",
    "    logger = ProcessRollingLogger(\n",
    "        # This should match the model type in WhyLabs. We're using a daily model here.\n",
    "        aggregate_by=TimeGranularity.Day,\n",
    "        # The profiles will be uploaded from the rolling logger to WhyLabs every 5 minutes. Data\n",
    "        # will accumulates during that time.\n",
    "        write_schedule=Schedule(cadence=TimeGranularity.Minute, interval=5),\n",
    "        schema=llm_metrics.init(),\n",
    "    )\n",
    "\n",
    "    logger.start()\n",
    "    return logger\n",
    "\n",
    "\n",
    "logger = create_logger()\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"\n",
    "    Load the model for inference\n",
    "    \"\"\"\n",
    "\n",
    "    # Load GPT2 tokenizer from disk.\n",
    "    vocab_path = os.path.join(model_dir, 'model/vocab.json')\n",
    "    merges_path = os.path.join(model_dir, 'model/merges.txt')\n",
    "    \n",
    "    tokenizer = GPT2Tokenizer(vocab_file=vocab_path,\n",
    "                              merges_file=merges_path)\n",
    "\n",
    "    # Load GPT2 model from disk.\n",
    "    model_path = os.path.join(model_dir, 'model/')\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "\n",
    "    return TextGenerationPipeline(model=model, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    \"\"\"\n",
    "    Apply model to the incoming request\n",
    "    \"\"\"\n",
    "\n",
    "    return model.__call__(input_data)\n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "    assert request_content_type == 'application/json'\n",
    "    return json.loads(request_body)\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    if 'flush' in input_data:\n",
    "        # Utility for flushing the logger, which forces it to upload any pending profiles synchronously.\n",
    "        print('>> flushing')\n",
    "        logger.flush()\n",
    "        return 'flushed'\n",
    "\n",
    "    if 'close' in input_data:\n",
    "        print('>> closing')\n",
    "        logger.close()\n",
    "        return 'closed'\n",
    "\n",
    "    output = model.__call__(input_data, max_length=100)\n",
    "\n",
    "    try:\n",
    "        # Log image async with whylogs. This won't hold up predictions.\n",
    "        print(f'>> prompt: ${input_data}')\n",
    "        logger.log({'prompt': input_data, 'response': output})  \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to log image: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "    return output\n",
    "\n",
    "def output_fn(prediction, content_type):\n",
    "    return str(prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a pytorch deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sagemaker import Predictor\n",
    "\n",
    "instance_type='ml.m5.xlarge'\n",
    "\n",
    "sagemaker_model = PyTorchModel(\n",
    "    source_dir='code',\n",
    "    entry_point='inference.py',\n",
    "    predictor_cls=Predictor,\n",
    "    model_data=upload_path,\n",
    "    framework_version='2.0',\n",
    "    py_version='py310',\n",
    "    role=aws_role,\n",
    "    env={\n",
    "        'WHYLABS_API_KEY': os.environ['WHYLABS_API_KEY'],\n",
    "        'WHYLABS_DEFAULT_DATASET_ID': os.environ['WHYLABS_DEFAULT_DATASET_ID']\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import StringDeserializer\n",
    "\n",
    "predictor = sagemaker_model.deploy(initial_instance_count=1, instance_type=instance_type)\n",
    "predictor.serializer = JSONSerializer()\n",
    "predictor.deserializer = StringDeserializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    query_response = predictor.predict(\n",
    "        text,\n",
    "        {\n",
    "            \"ContentType\": \"application/json\",\n",
    "            \"Accept\": \"application/json\",\n",
    "        },\n",
    "    )\n",
    "    return query_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\"LLMs are... \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Force the logger to upload\n",
    "\n",
    "> ⚠️ These things really only reliably work if you have a single instance behind your prediction endpoint. Otherwise you'll have to make sure these requests get to each endpoint individually.\n",
    "\n",
    "This forces the logger to upload (see the inference.py code) which uploads any remaining data in the logger before we close down the Sagemaker endpoint. The rolling logger typically uploads data on a predefined interval so you can do something like this to make sure you don't clip your profile uploads before shutting things down. Sagemaker doesn't provide any \"on close\" hooks to make this transparent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'flushed'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('flush')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, you can close the logger. This also forces an upload but results in the logger no longer being active, so you should only do this before you're about to tear down the endpoint since it let's you synchronously wait for any pending uploads to finish.\n",
    "\n",
    "Remember, these don't automatically work. They only work because we set up the `inference.py`` file to check for these payloads and call the right methods on the logger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'closed'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict('close')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
